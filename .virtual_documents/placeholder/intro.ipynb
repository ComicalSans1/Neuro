import numpy as np
import matplotlib.pyplot as plt
import scipy
import scipy.stats as stats
get_ipython().run_line_magic("matplotlib", " inline ")

import io
import requests
r = requests.get('https://osf.io/sy5xt/download')
if r.status_code != 200:
  print('Failed to download data')
else:
  spike_times = np.load(io.BytesIO(r.content), allow_pickle=True)['spike_times']


spike_counts = [len(x) for x in spike_times]
# print(type(spike_times), type(spike_times[0]))
# array of arrays, each with the data for one neuron across the time period.\
mean_spikes = np.mean(spike_counts)
median_spikes = np.median(spike_counts)
print(f"Mean number of spikes: {mean_spikes:.2f}")
print(f"Median number of spikes: {median_spikes:.2f}")
print(f"Maximum number of spikes: {np.max(spike_counts):.2f}")
plt.hist(spike_counts, bins=100);
plt.axvline(mean_spikes, color="orange", label="Mean");
# Exercise 1.2:
plt.axvline(median_spikes, color="lime", label="Median");
# Q1 = np.percentile(spike_counts, 25)
# Q3 = np.percentile(spike_counts, 75)
# plt.axvline(Q1, color="black")
# plt.axvline(Q3, color="black")


all_intervals = []

def restrict_spike_intervals(spike_times, s_min, s_max):
    for s_t in spike_times:
        restricted_intervals = [n for n in s_t if n >= s_min and n <= s_max]
        all_intervals.append(restricted_intervals)
    return all_intervals
    
restricted_spike_intervals = restrict_spike_intervals(spike_times, 5, 15)


all_restricted_intervals = np.concatenate(restricted_spike_intervals)

print("Length of all intervals:", len(np.concatenate(spike_times)))
print("Length of restricted:", len(all_restricted_intervals))
print(f"Mean interval: {np.mean(all_restricted_intervals):.2f}")
print(f"Median interval: {np.median(all_restricted_intervals):.2f}")
print(f"Maximum interval: {np.max(all_restricted_intervals):.2f}")


print(f"Total percent of all intervals in this range: {len(all_restricted_intervals) / len(np.concatenate(spike_times)) * 100:.2f}%")
spike_times_flat = np.concatenate(spike_times)
experiment_duration = np.ptp(spike_times_flat) # uses np's point-to-point function to fins the maximum interval in the whole thing
interval_duration = 10

frac_interval_time = interval_duration / experiment_duration
print(f"{frac_interval_time:.2%} of the total time is in the interval")
# oh man, close enough i guess


restricted_spike_intervals = np.array(restricted_spike_intervals, object)
neuron_idx = np.arange(0, len(spike_times))
plt.eventplot(restricted_spike_intervals[neuron_idx], color=".2");


def exponential(xs, scale, rate, x0):
    ys = scale * np.exp(rate * (xs - x0))
    return ys

x_range = np.linspace(0.018, single_neuron_interval.max(), 100)
y_values = exponential(x_range, 5000, -55, 0.04)

plt.hist(single_neuron_interval, bins=50, alpha=0.7, label='Data');
plt.plot(x_range, y_values, color='red');
plt.axvline(np.mean(single_neuron_interval), color="orange", linestyle='--', label='Mean');


# import torch
# import torch.nn as nn

# model = 

# y_values
# counts, edges = np.histogram(single_neuron_interval, bins=50)
# centers = (edges[:-1] + edges[1:]) / 2


import scipy
import scipy.stats as stats

def lif_neuron(n_steps=1000, alpha=0.5, beta=0.1, a_rate=10, b_rate=10):
    """ Simulates a leaky integrate-and-fire model of the neuron.

        the equation: dv = alpha * (exc[i] - inh[i]) - beta * v[i-1]
        "exc" and "inh" respectively, represent excitatory and inhibitory signals fired by presynaptic neurons. 
        These signals are simulated here using a Poisson distribution.
        To simulate passive leakage of charge across the membrane, we also have a leak current represented by leakage rate beta times the voltage of the membrane.
        
        The spiking frequency follows an exponential distribution because the firing probability of a neuron in a time period dt 
        is independent of previous time intervals, and so satisfies the memoryless property needed for exponential distributions?
    """
    exc = stats.poisson(a_rate).rvs(n_steps)
    inh = stats.poisson(b_rate).rvs(n_steps)

    v = np.zeros(n_steps)
    spike_times = []

    for i in range(1, n_steps):
        dv = alpha * (exc[i] - inh[i]) - beta * v[i-1]
        v[i] = v[i-1] + dv
      
        if v[i] > 1:
            spike_times.append(i)
            v[i] = 0

    return v, spike_times

np.random.seed(12)
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))

v, spike_times = lif_neuron()

ax1.plot(v[0:100])
for x in spike_times:
    if x <= 100:
        ax1.axvline(x, color="red")

ax2.hist([t - s for s, t in zip(spike_times[:-1], spike_times[1:])], np.linspace(0, 20))

plt.show()

"""
1. Spikes at regular intervals
2. Higher alpha means more spikes. Makes sense because higher currents means higher excitation of the neuron, leading to a higher spike frequency.
3. Higher rate increases the rate of firing. Big surprise there
4. No. This means it follows a different fundamental type of function to generate spikes?
""";


fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 3))

n_bins = 50  # number of points supporting the distribution
bins = np.linspace(*x_range, n_bins )  # bin edges

pmf = np.zeros(n_bins)
pmf[len(pmf) // 2] = 1.0  # middle point has all the mass

pmf_1 = np.zeros(n_bins)
pmf_1[len(pmf_1) // 3] = pmf_1[2 * len(pmf_1) // 3] = 0.5

pmf_2 = np.array([1/n_bins] * n_bins)

ax1.plot(bins, pmf, drawstyle="steps")
ax1.fill_between(bins, pmf, step="pre", alpha=0.4)

ax2.plot(bins, pmf_1, drawstyle="steps")
ax2.fill_between(bins, pmf_1, step="pre", alpha=0.4)

ax3.plot(bins, pmf_2)
plt.xlim(0, 1)
plt.ylim(0, 1)
plt.show()


def entropy(pmf):
    """ Regarding biological neurons, one objective regarding interspike intervals is: maximize the information transmitted by ISIs.
        The definition of information we use here is the Shannon entropy. -sum(p * log2(p)).
    """
    total_entropy = 0
    for p in pmf:
        if p != 0:
            total_entropy -= p * np.log2(p)
    return total_entropy

print(entropy(pmf)) # 0 entropy, as only 1 value is possible.
print(entropy(pmf_1)) # -(0.5log2(0.5) + 0.5log2(0.5)) = 1
print(f"{entropy(pmf_2):.2f}") # entropy of a uniform distribution over n values is simply log2(n)

"""
    Here, we find that the maximally entropic distribution is a uniform distribution. 
    This is inconvenient, because for large intervals it's just infenitesimally small probs across an infinite distance.
    Thus, the implied constraint is that each spike costs some energy to produce.
    The objective can then be reframed as maximixation of entropy *given a fixed number of spikes.*
""";


n_bins = 50
mean_isi = 0.025 # fixing the mean isi is == to fixing the mean firing rate, which allows us to optimize for our objective

bins = np.linspace(0, 0.5, n_bins + 1)
mean_idx = np.searchsorted(bins, mean_isi)

# 1. all mass concentrated on the ISI mean
pmf_single = np.zeros(n_bins)
pmf_single[mean_idx] = 1.0

# 2. mass uniformly distributed about the ISI mean
pmf_uniform = np.zeros(n_bins)
upper_bound_idx = np.argmin(np.abs(bin_centers - (2 * mean_isi)))
pmf_uniform[:upper_bound_idx] = 1.0 / upper_bound_idx

# 3. mass exponentially distributed about the ISI mean
pmf_exp = stats.expon.pdf(bins, scale=mean_isi)
pmf_exp /= pmf_exp.sum() # Normalize to ensure it sums to 1

print(entropy(pmf_single))
print(entropy(pmf_uniform))
print(entropy(pmf_exp))


plt.plot(pmf_exp, drawstyle="steps")
plt.plot(pmf_single, drawstyle="steps")
plt.plot(pmf_uniform, drawstyle="steps")
plt.ylim(0, 1.0)



